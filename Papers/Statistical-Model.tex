\section*{Statistical Model }
Hedonic regression is a specialized application of linear regression in which the price of a differentiated good is decomposed into the effect of various attributes. Although the underlying estimation method remains the method of least squares, the key distinction lies in the interpretation: each coefficient in a hedonic model represents the implicit price of a specific characteristic holding all else constant. 

\subsection*{Log-linear Hedonic Regression}
Linear regression is well-suited for this framework because it allows one to estimate how marginal changes in features affect market value. However, since housing attributes often exhibit non-constant effects (particularly diminishing marginal returns) a log-linear specification is typically preferred \citep{abelson:1979}. The log-linear hedonic model takes the following form: 

\[
\log P_n = \beta_0 + \mathbf{w}_n \boldsymbol\gamma + \mathbf{z}_n \boldsymbol\delta + U_n
\]

\noindent where  \( \log P_n \) is a natural logarithm of the sale price for property \(n\), \( \beta_0 \) is an intercept term, \( \mathbf{w}_n \) is a vector of structural characteristics, \( \boldsymbol\gamma \) is a vector of coefficients for structural characteristics, \( \mathbf{z}_n \) is a vector of neighborhood characteristics, \( \boldsymbol\delta \) is a vector of coefficients for neighborhood characteristics, \( U_n \) is an error term capturing unobserved influences for property \(n\).


\subsection*{LASSO Regularization}

Although the log-linear hedonic specification effectively captures the marginal contributions of housing attributes, models with a large number of correlated predictors risk overfitting, which can destabilize coefficient estimates and impair interpretability. Least Absolute Shrinkage and Selection Operator (LASSO) regularization technique can be employed to improve model's generalizability and addresses multicollinearity by prioritizing variables that contribute most to explaining variation in dependent variable.

LASSO augments the standard sum-of-squares objective function with an additional penalty term that imposes an \( \ell_1 \)-norm constraint on the regression coefficients. The optimization problem is defined by the following:

\[
L(\hat{\boldsymbol{\beta}}) = \arg\min_{\boldsymbol\beta}\; \sum_{n=1}^{N} \left( \log P_n - \beta_0 - \mathbf{x}_n^s \boldsymbol \beta \right)^2 + \mu \left\| \boldsymbol\beta_{-1} \right\|_1 
\]

\noindent where \( \log P_n \) is the observed outcome for observation $n$, \( \mathbf{x}_n^s \) denotes the vector of standardized predictors for observation $n$, \( \boldsymbol{\beta} \) represents the vector of coefficients, and \( \mu  \) is a tuning parameter that controls the degree of penalization. Here, $\beta_{-1}$ denotes the vector $\beta$ without the constant $\beta_0$, and the penalty term \( \| \boldsymbol\beta_{-1} \|_1 \) shrinks some coefficients toward zero, thereby performing implicit variable selection. 

\subsection*{Linear Generalized Additive Model}

Although linear regression assumes constant marginal effects for each covariate, the generalized additive model (GAM) relaxes this assumption by allowing the effect of selected continuous variables to vary smoothly over their range. This approach preserves interpretability for linear terms while enabling the model to uncover nonlinear patterns in the data that a single index f the feature variables may fail to detect. 

\[
\log P_n = \beta_0 + \sum_{j} \beta_j x_{nj} + \sum_{k} f_k(z_{nk}) + U_n
\]

\noindent where  \( \ln p_n \) is a natural logarithm of the sale price for property \(n\), \( \beta_0 \) is an intercept term, \( x_{nj} \) are covariates with linear effects for observation \( n \), \( \beta_j \) are the coefficients corresponding to the linear covariates, \( f_k(\cdot) \) are smooth functions represented by penalized splines, estimated for continuous covariates \( z_{nk} \), and \( U_n \) is an error term capturing unobserved influences.
